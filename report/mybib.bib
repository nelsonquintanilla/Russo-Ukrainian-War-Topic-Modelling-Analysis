@article{blei_probabilistic_2012,
	title = {Probabilistic topic models},
	volume = {55},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2133806.2133826},
	doi = {10.1145/2133806.2133826},
	abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
	language = {en},
	number = {4},
	urldate = {2022-09-16},
	journal = {Communications of the ACM},
	author = {Blei, David M.},
	month = apr,
	year = {2012},
	pages = {77--84},
	file = {Blei - 2012 - Probabilistic topic models.pdf:/Users/nelsonquintanilla/Zotero/storage/I2J2PVHI/Blei - 2012 - Probabilistic topic models.pdf:application/pdf},
}

@article{blei_probabilistic_2010,
	title = {Probabilistic {Topic} {Models}},
	volume = {27},
	issn = {1558-0792},
	doi = {10.1109/MSP.2010.938079},
	abstract = {In this article, we review probabilistic topic models: graphical models that can be used to summarize a large collection of documents with a smaller number of distributions over words. Those distributions are called "topics" because, when fit to data, they capture the salient themes that run through the collection. We describe both finite-dimensional parametric topic models and their Bayesian nonparametric counterparts, which are based on the hierarchical Dirichlet process (HDP). We discuss two extensions of topic models to time-series data-one that lets the topics slowly change over time and one that lets the assumed prevalence of the topics change. Finally, we illustrate the application of topic models to nontext data, summarizing some recent research results in image analysis.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Blei, David and Carin, Lawrence and Dunson, David},
	month = nov,
	year = {2010},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Analytical models, Bayesian methods, Computational modeling, Data models, Graphical models, Markov processes},
	pages = {55--65},
	file = {Blei - Probabilistic topic models.pdf:/Users/nelsonquintanilla/Zotero/storage/MAPDH4FW/Blei - Probabilistic topic models.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/nelsonquintanilla/Zotero/storage/XL5BIT9W/5563111.html:text/html;IEEE Xplore Full Text PDF:/Users/nelsonquintanilla/Zotero/storage/4CXA3DRX/Blei et al. - 2010 - Probabilistic Topic Models.pdf:application/pdf},
}

@article{blei_latent_nodate,
	title = {Latent {Dirichlet} {Allocation}},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a ﬁnite mixture over an underlying set of topics. Each topic is, in turn, modeled as an inﬁnite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efﬁcient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classiﬁcation, and collaborative ﬁltering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	language = {en},
	author = {Blei, David M},
	pages = {30},
	file = {Blei - Latent Dirichlet Allocation.pdf:/Users/nelsonquintanilla/Zotero/storage/VXMXVDHN/Blei - Latent Dirichlet Allocation.pdf:application/pdf},
}

@inproceedings{sievert_ldavis_2014,
	address = {Baltimore, Maryland, USA},
	title = {{LDAvis}: {A} method for visualizing and interpreting topics},
	shorttitle = {{LDAvis}},
	url = {http://aclweb.org/anthology/W14-3110},
	doi = {10.3115/v1/W14-3110},
	abstract = {We present LDAvis, a web-based interactive visualization of topics estimated using Latent Dirichlet Allocation that is built using a combination of R and D3. Our visualization provides a global view of the topics (and how they differ from each other), while at the same time allowing for a deep inspection of the terms most highly associated with each individual topic. First, we propose a novel method for choosing which terms to present to a user to aid in the task of topic interpretation, in which we deﬁne the relevance of a term to a topic. Second, we present results from a user study that suggest that ranking terms purely by their probability under a topic is suboptimal for topic interpretation. Last, we describe LDAvis, our visualization system that allows users to ﬂexibly explore topic-term relationships using relevance to better understand a ﬁtted LDA model.},
	language = {en},
	urldate = {2022-08-31},
	booktitle = {Proceedings of the {Workshop} on {Interactive} {Language} {Learning}, {Visualization}, and {Interfaces}},
	publisher = {Association for Computational Linguistics},
	author = {Sievert, Carson and Shirley, Kenneth},
	year = {2014},
	pages = {63--70},
	file = {Sievert and Shirley - 2014 - LDAvis A method for visualizing and interpreting .pdf:/Users/nelsonquintanilla/Zotero/storage/E4S6JPWG/Sievert and Shirley - 2014 - LDAvis A method for visualizing and interpreting .pdf:application/pdf},
}

@article{jacobi_quantitative_2016,
	title = {Quantitative analysis of large amounts of journalistic texts using topic modelling},
	volume = {4},
	issn = {2167-0811, 2167-082X},
	url = {http://www.tandfonline.com/doi/full/10.1080/21670811.2015.1093271},
	doi = {10.1080/21670811.2015.1093271},
	language = {en},
	number = {1},
	urldate = {2022-09-17},
	journal = {Digital Journalism},
	author = {Jacobi, Carina and van Atteveldt, Wouter and Welbers, Kasper},
	month = jan,
	year = {2016},
	keywords = {automatic content analysis, Corrigendum, journalism, nuclear energy, topic models},
	pages = {89--106},
	file = {Jacobi et al. - 2016 - Quantitative analysis of large amounts of journali.pdf:/Users/nelsonquintanilla/Zotero/storage/GM26PM6G/Jacobi et al. - 2016 - Quantitative analysis of large amounts of journali.pdf:application/pdf;Snapshot:/Users/nelsonquintanilla/Zotero/storage/F4NA2GZ4/21670811.2015.html:text/html},
}

@article{caldara_effect_2022,
	title = {The {Effect} of the {War} in {Ukraine} on {Global} {Activity} and {Inflation}},
	url = {https://www.federalreserve.gov/econres/notes/feds-notes/the-effect-of-the-war-in-ukraine-on-global-activity-and-inflation-20220527.html},
	abstract = {The Federal Reserve Board of Governors in Washington DC.},
	language = {en},
	urldate = {2022-09-17},
	author = {Caldara, Dario and Conlisk, Sarah and Iacoviello, Matteo and Penn, Maddie},
	month = may,
	year = {2022},
	file = {Snapshot:/Users/nelsonquintanilla/Zotero/storage/C76JDUYI/the-effect-of-the-war-in-ukraine-on-global-activity-and-inflation-20220527.html:text/html},
}

@article{jelodar_latent_2019,
	title = {Latent {Dirichlet} allocation ({LDA}) and topic modeling: models, applications, a survey},
	volume = {78},
	issn = {1380-7501, 1573-7721},
	shorttitle = {Latent {Dirichlet} allocation ({LDA}) and topic modeling},
	url = {http://link.springer.com/10.1007/s11042-018-6894-4},
	doi = {10.1007/s11042-018-6894-4},
	abstract = {Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data and text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modelling; Latent Dirichlet Allocation (LDA) is one of the most popular in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper will be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated highly scholarly articles (between 2003 to 2016) related to topic modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. In addition, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA.},
	language = {en},
	number = {11},
	urldate = {2022-09-17},
	journal = {Multimedia Tools and Applications},
	author = {Jelodar, Hamed and Wang, Yongli and Yuan, Chi and Feng, Xia and Jiang, Xiahui and Li, Yanchao and Zhao, Liang},
	month = jun,
	year = {2019},
	pages = {15169--15211},
	file = {Jelodar et al. - 2019 - Latent Dirichlet allocation (LDA) and topic modeli.pdf:/Users/nelsonquintanilla/Zotero/storage/NW5KNSTF/Jelodar et al. - 2019 - Latent Dirichlet allocation (LDA) and topic modeli.pdf:application/pdf},
}

@article{nikolenko_topic_2017,
	title = {Topic modelling for qualitative studies},
	volume = {43},
	issn = {0165-5515},
	url = {https://doi.org/10.1177/0165551515617393},
	doi = {10.1177/0165551515617393},
	abstract = {Qualitative studies, such as sociological research, opinion analysis and media studies, can benefit greatly from automated topic mining provided by topic models such as latent Dirichlet allocation (LDA). However, examples of qualitative studies that employ topic modelling as a tool are currently few and far between. In this work, we identify two important problems along the way to using topic models in qualitative studies: lack of a good quality metric that closely matches human judgement in understanding topics and the need to indicate specific subtopics that a specific qualitative study may be most interested in mining. For the first problem, we propose a new quality metric, tf-idf coherence, that reflects human judgement more accurately than regular coherence, and conduct an experiment to verify this claim. For the second problem, we propose an interval semi-supervised approach (ISLDA) where certain predefined sets of keywords (that define the topics researchers are interested in) are restricted to specific intervals of topic assignments. Our experiments show that ISLDA is better for topic extraction than LDA in terms of tf-idf coherence, number of topics identified to predefined keywords and topic stability. We also present a case study on a Russian LiveJournal dataset aimed at ethnicity discourse analysis.},
	language = {en},
	number = {1},
	urldate = {2022-07-11},
	journal = {Journal of Information Science},
	author = {Nikolenko, Sergey I. and Koltcov, Sergei and Koltsova, Olessia},
	month = feb,
	year = {2017},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {Latent Dirichlet allocation, LDA extensions, topic modelling, topic quality},
	pages = {88--102},
	file = {SAGE PDF Full Text:/Users/nelsonquintanilla/Zotero/storage/QXV8NXVP/Nikolenko et al. - 2017 - Topic modelling for qualitative studies.pdf:application/pdf},
}

@article{chang_reading_nodate,
	title = {Reading {Tea} {Leaves}: {How} {Humans} {Interpret} {Topic} {Models}},
	abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
	language = {en},
	author = {Chang, Jonathan and Boyd-Graber, Jordan and Gerrish, Sean and Wang, Chong and Blei, David M},
	pages = {10},
	file = {Chang et al. - Reading Tea Leaves How Humans Interpret Topic Mod.pdf:/Users/nelsonquintanilla/Zotero/storage/8W2NEUPZ/Chang et al. - Reading Tea Leaves How Humans Interpret Topic Mod.pdf:application/pdf},
}

@misc{matsa_western_2018,
	title = {Western {Europeans} {Under} 30 {View} {News} {Media} {Less} {Positively}, {Rely} {More} on {Digital} {Platforms} {Than} {Older} {Adults}},
	url = {https://www.pewresearch.org/journalism/2018/10/30/western-europeans-under-30-view-news-media-less-positively-rely-more-on-digital-platforms-than-older-adults/},
	abstract = {Younger adults in eight Western European countries are about twice as likely as older adults to get news online than from TV. They also are more critical of the media's performance and coverage of key issues.},
	language = {en-US},
	urldate = {2022-09-22},
	journal = {Pew Research Center's Journalism Project},
	author = {Matsa, Katerina Eva and Silver, Laura and Shearer, Elisa and Walker, Mason},
	month = oct,
	year = {2018},
	file = {Snapshot:/Users/nelsonquintanilla/Zotero/storage/BYK48N7C/western-europeans-under-30-view-news-media-less-positively-rely-more-on-digital-platforms-than-.html:text/html},
}

@misc{noauthor_theguardian_nodate,
	title = {theguardian / open platform - documentation / overview},
	url = {https://open-platform.theguardian.com/documentation/},
	urldate = {2022-09-22},
	file = {theguardian / open platform - documentation / overview:/Users/nelsonquintanilla/Zotero/storage/UXCUQDHN/documentation.html:text/html},
}

@article{jacobi_quantitative_2016,
	title = {Quantitative analysis of large amounts of journalistic texts using topic modelling},
	volume = {4},
	issn = {2167-0811, 2167-082X},
	url = {http://www.tandfonline.com/doi/full/10.1080/21670811.2015.1093271},
	doi = {10.1080/21670811.2015.1093271},
	language = {en},
	number = {1},
	urldate = {2022-09-17},
	journal = {Digital Journalism},
	author = {Jacobi, Carina and van Atteveldt, Wouter and Welbers, Kasper},
	month = jan,
	year = {2016},
	keywords = {automatic content analysis, Corrigendum, journalism, nuclear energy, topic models},
	pages = {89--106},
	file = {Jacobi et al. - 2016 - Quantitative analysis of large amounts of journali.pdf:/Users/nelsonquintanilla/Zotero/storage/GM26PM6G/Jacobi et al. - 2016 - Quantitative analysis of large amounts of journali.pdf:application/pdf;Snapshot:/Users/nelsonquintanilla/Zotero/storage/F4NA2GZ4/21670811.2015.html:text/html},
}

@article{bird_natural_nodate,
	title = {Natural {Language} {Processing} with {Python}},
	language = {en},
	author = {Bird, Steven},
	pages = {504},
	file = {Bird - Natural Language Processing with Python.pdf:/Users/nelsonquintanilla/Zotero/storage/D558Z82Q/Bird - Natural Language Processing with Python.pdf:application/pdf},
}

@article{pedregosa_scikit-learn_nodate,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simpliﬁed BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	language = {en},
	journal = {MACHINE LEARNING IN PYTHON},
	author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David},
	pages = {6},
	file = {Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf:/Users/nelsonquintanilla/Zotero/storage/DFYC4M3V/Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf:application/pdf},
}

@unpublished{spacy2,
    AUTHOR = {Honnibal, Matthew and Montani, Ines},
    TITLE  = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing},
    YEAR   = {2017},
    Note   = {To appear}
}

@book{rajaraman_mining_2011,
	address = {USA},
	title = {Mining of {Massive} {Datasets}},
	isbn = {978-1-107-01535-7},
	abstract = {The popularity of the Web and Internet commerce provides many extremely large datasets from which information can be gleaned by data mining. This book focuses on practical algorithms that have been used to solve key problems in data mining and which can be used on even the largest datasets. It begins with a discussion of the map-reduce framework, an important tool for parallelizing algorithms automatically. The authors explain the tricks of locality-sensitive hashing and stream processing algorithms for mining data that arrives too fast for exhaustive processing. The PageRank idea and related tricks for organizing the Web are covered next. Other chapters cover the problems of finding frequent itemsets and clustering. The final chapters cover two applications: recommendation systems and Web advertising, each vital in e-commerce. Written by two authorities in database and Web technologies, this book is essential reading for students and practitioners alike.},
	publisher = {Cambridge University Press},
	author = {Rajaraman, Anand and Ullman, Jeffrey David},
	year = {2011},
}

@article{rehurek_gensimstatistical_nodate,
	title = {Gensim—{Statistical} {Semantics} in {Python}},
	abstract = {Gensim is a pure Python library that fights on two fronts: 1) digital document indexing and similarity search; and 2) fast, memory-efficient, scalable algorithms for Singular Value Decomposition and Latent Dirichlet Allocation. The connection between the two is unsupervised, semantic analysis of plain text in digital collections. Gensim was created for large digital libraries, but its underlying algorithms for large-scale, distributed, online SVD and LDA are like the Swiss Army knife of data analysis—also useful on their own, outside of the domain of Natural Language Processing.},
	language = {en},
	author = {Řehůřek, Radim and Sojka, Petr},
	pages = {1},
	file = {Řehůřek and Sojka - Gensim—Statistical Semantics in Python.pdf:/Users/nelsonquintanilla/Zotero/storage/7VKWSNHU/Řehůřek and Sojka - Gensim—Statistical Semantics in Python.pdf:application/pdf},
}

@INPROCEEDINGS{Rehurek10softwareframework,
    author = {Radim Rehurek and Petr Sojka},
    title = {Software Framework for Topic Modelling with Large Corpora},
    booktitle = {IN PROCEEDINGS OF THE LREC 2010 WORKSHOP ON NEW CHALLENGES FOR NLP FRAMEWORKS},
    year = {2010},
    pages = {45--50},
    publisher = {}
}

@inproceedings{2009-topic-modeling-social-sciences,
  title = {Topic Modeling for the Social Sciences},
  author = {Daniel Ramage AND Evan Rosen AND Jason Chuang AND Christopher D. Manning AND Daniel A. McFarland},
  booktitle = {Workshop on Applications for Topic Models, NIPS},
  year = {2009},
  url = {http://vis.stanford.edu/papers/topic-modeling-social-sciences}
}

@unpublished{McCallumMALLET,
      author = "Andrew Kachites McCallum",
      title = "MALLET: A Machine Learning for Language Toolkit",
      note = "http://www.cs.umass.edu/~mccallum/mallet",
      year = 2002}
      
@article{blei_probabilistic_2010,
	title = {Probabilistic {Topic} {Models}},
	volume = {27},
	issn = {1558-0792},
	doi = {10.1109/MSP.2010.938079},
	abstract = {In this article, we review probabilistic topic models: graphical models that can be used to summarize a large collection of documents with a smaller number of distributions over words. Those distributions are called "topics" because, when fit to data, they capture the salient themes that run through the collection. We describe both finite-dimensional parametric topic models and their Bayesian nonparametric counterparts, which are based on the hierarchical Dirichlet process (HDP). We discuss two extensions of topic models to time-series data-one that lets the topics slowly change over time and one that lets the assumed prevalence of the topics change. Finally, we illustrate the application of topic models to nontext data, summarizing some recent research results in image analysis.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Blei, David and Carin, Lawrence and Dunson, David},
	month = nov,
	year = {2010},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Analytical models, Bayesian methods, Computational modeling, Data models, Graphical models, Markov processes},
	pages = {55--65},
	file = {IEEE Xplore Abstract Record:/Users/nelsonquintanilla/Zotero/storage/CQTEH3T8/5563111.html:text/html;IEEE Xplore Full Text PDF:/Users/nelsonquintanilla/Zotero/storage/9RZMGA3J/Blei et al. - 2010 - Probabilistic Topic Models.pdf:application/pdf},
}

@article{asmussen_smart_2019,
	title = {Smart literature review: a practical topic modelling approach to exploratory literature review},
	volume = {6},
	copyright = {2019 The Author(s)},
	issn = {2196-1115},
	shorttitle = {Smart literature review},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0255-7},
	doi = {10.1186/s40537-019-0255-7},
	abstract = {Manual exploratory literature reviews should be a thing of the past, as technology and development of machine learning methods have matured. The learning curve for using machine learning methods is rapidly declining, enabling new possibilities for all researchers. A framework is presented on how to use topic modelling on a large collection of papers for an exploratory literature review and how that can be used for a full literature review. The aim of the paper is to enable the use of topic modelling for researchers by presenting a step-by-step framework on a case and sharing a code template. The framework consists of three steps; pre-processing, topic modelling, and post-processing, where the topic model Latent Dirichlet Allocation is used. The framework enables huge amounts of papers to be reviewed in a transparent, reliable, faster, and reproducible way.},
	language = {en},
	number = {1},
	urldate = {2022-07-05},
	journal = {Journal of Big Data},
	author = {Asmussen, Claus Boye and Møller, Charles},
	month = dec,
	year = {2019},
	note = {Number: 1
Publisher: SpringerOpen},
	pages = {1--18},
	file = {Full Text PDF:/Users/nelsonquintanilla/Zotero/storage/UBYC6ZDN/Asmussen and Møller - 2019 - Smart literature review a practical topic modelli.pdf:application/pdf;Snapshot:/Users/nelsonquintanilla/Zotero/storage/HBD379GE/s40537-019-0255-7.html:text/html},
}


@inproceedings{xu_probabilistic_2016,
	address = {New York, NY, USA},
	series = {{DSMM}'16},
	title = {Probabilistic {Financial} {Community} {Models} with {Latent} {Dirichlet} {Allocation} for {Financial} {Supply} {Chains}},
	isbn = {978-1-4503-4407-4},
	url = {https://doi.org/10.1145/2951894.2951900},
	doi = {10.1145/2951894.2951900},
	abstract = {There is a growing interest in modeling and predicting the behavior of financial systems and supply chains. In this paper, we focus on the the analysis of the resMBS supply chain; it is associated with the US residential mortgage backed securities and subprime mortgages that were critical in the 2008 US financial crisis. We develop models based on financial institutions (FI), and their participation described by their roles (Role) on financial contracts (FC). Our models are based on an intuitive assumption that FIs will form communities within an FC, and FIs within a community are more likely to collaborate with other FIs in that community, and play the same role, in another FC. Inspired by the Latent Dirichlet Allocation (LDA) and topic models, we develop two probabilistic financial community models. In FI-Comm, each FC (document) is a mix of topics where a topic is a distribution over FIs (words). In Role-FI-Comm, each topic is a distribution over Role-FI pairs (words). Experimental results over 5000+ financial prospecti demonstrate the effectiveness of our models.},
	urldate = {2022-09-22},
	booktitle = {Proceedings of the {Second} {International} {Workshop} on {Data} {Science} for {Macro}-{Modeling}},
	publisher = {Association for Computing Machinery},
	author = {Xu, Zheng and Raschid, Louiqa},
	month = jun,
	year = {2016},
	keywords = {financial supply chain, generative probabilistic model, Latent Dirichlet allocation, mortgage backed securities},
	pages = {1--6},
	file = {Full Text PDF:/Users/nelsonquintanilla/Zotero/storage/9TWB29TS/Xu and Raschid - 2016 - Probabilistic Financial Community Models with Late.pdf:application/pdf},
}


@article{blei_correlated_2007,
	title = {A correlated topic model of {Science}},
	volume = {1},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-1/issue-1/A-correlated-topic-model-of-Science/10.1214/07-AOAS114.full},
	doi = {10.1214/07-AOAS114},
	abstract = {Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than X-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982) 139–177]. We derive a fast variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. We apply the CTM to the articles from Science published from 1990–1999, a data set that comprises 57M words. The CTM gives a better fit of the data than LDA, and we demonstrate its use as an exploratory tool of large document collections.},
	number = {1},
	urldate = {2022-09-22},
	journal = {The Annals of Applied Statistics},
	author = {Blei, David M. and Lafferty, John D.},
	month = jun,
	year = {2007},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {approximate posterior inference, hierarchical models, text analysis, variational methods},
	pages = {17--35},
	file = {Full Text PDF:/Users/nelsonquintanilla/Zotero/storage/M6XJGRQL/Blei and Lafferty - 2007 - A correlated topic model of Science.pdf:application/pdf;Snapshot:/Users/nelsonquintanilla/Zotero/storage/PXVY5BXI/07-AOAS114.html:text/html},
}

@article{zhao_heuristic_2015,
	title = {A heuristic approach to determine an appropriate number of topics in topic modeling},
	volume = {16},
	copyright = {2015 Zhao et al.},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-16-S13-S8},
	doi = {10.1186/1471-2105-16-S13-S8},
	abstract = {Topic modelling is an active research field in machine learning. While mainly used to build models from unstructured textual data, it offers an effective means of data mining where samples represent documents, and different biological endpoints or omics data represent words. Latent Dirichlet Allocation (LDA) is the most commonly used topic modelling method across a wide number of technical fields. However, model development can be arduous and tedious, and requires burdensome and systematic sensitivity studies in order to find the best set of model parameters. Often, time-consuming subjective evaluations are needed to compare models. Currently, research has yielded no easy way to choose the proper number of topics in a model beyond a major iterative approach. Based on analysis of variation of statistical perplexity during topic modelling, a heuristic approach is proposed in this study to estimate the most appropriate number of topics. Specifically, the rate of perplexity change (RPC) as a function of numbers of topics is proposed as a suitable selector. We test the stability and effectiveness of the proposed method for three markedly different types of grounded-truth datasets: Salmonella next generation sequencing, pharmacological side effects, and textual abstracts on computational biology and bioinformatics (TCBB) from PubMed. The proposed RPC-based method is demonstrated to choose the best number of topics in three numerical experiments of widely different data types, and for databases of very different sizes. The work required was markedly less arduous than if full systematic sensitivity studies had been carried out with number of topics as a parameter. We understand that additional investigation is needed to substantiate the method's theoretical basis, and to establish its generalizability in terms of dataset characteristics.},
	language = {en},
	number = {13},
	urldate = {2022-09-22},
	journal = {BMC Bioinformatics},
	author = {Zhao, Weizhong and Chen, James J. and Perkins, Roger and Liu, Zhichao and Ge, Weigong and Ding, Yijun and Zou, Wen},
	month = dec,
	year = {2015},
	note = {Number: 13
Publisher: BioMed Central},
	pages = {1--10},
	file = {Full Text PDF:/Users/nelsonquintanilla/Zotero/storage/V5VKSDDF/Zhao et al. - 2015 - A heuristic approach to determine an appropriate n.pdf:application/pdf;Snapshot:/Users/nelsonquintanilla/Zotero/storage/KULACTAS/1471-2105-16-S13-S8.html:text/html},
}

@article{ray_review_2019,
	title = {Review and {Implementation} of {Topic} {Modeling} in {Hindi}},
	volume = {33},
	issn = {0883-9514},
	url = {https://doi.org/10.1080/08839514.2019.1661576},
	doi = {10.1080/08839514.2019.1661576},
	abstract = {Due to the widespread usage of electronic devices and the growing popularity of social media, a lot of text data is being generated at the rate never seen before. It is not possible for humans to read all data generated and find what is being discussed in his field of interest. Topic modeling is a technique to identify the topics present in a large set of text documents. In this paper, we have discussed the widely used techniques and tools for topic modeling. There has been a lot of research on topic modeling in English, but there is not much progress in the resource-scarce languages like Hindi despite Hindi being spoken by millions of people across the world. In this paper, we have discussed the challenges faced in developing topic models for Hindi. We have applied Latent Semantic Indexing (LSI), Non-negative Matrix Factorization (NMF), and Latent Dirichlet Allocation (LDA) algorithms for topic modeling in Hindi. The outcomes of the topic model algorithms are usually difficult to interpret for the common user. We have used various visualization techniques to represent the outcomes of topic modeling in a meaningful way. Then we have used the metrics like perplexity and coherence to evaluate the topic models. The results of Topic modeling in Hindi seem to be promising and comparable to some results reported in the literature on English datasets.},
	number = {11},
	urldate = {2022-09-23},
	journal = {Applied Artificial Intelligence},
	author = {Ray, Santosh Kumar and Ahmad, Amir and Kumar, Ch. Aswani},
	month = sep,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08839514.2019.1661576},
	pages = {979--1007},
}

@inproceedings{li_seeded-btm_2019,
	title = {Seeded-{BTM}: {Enabling} {Biterm} {Topic} {Model} with {Seeds} for {Product} {Aspect} {Mining}},
	shorttitle = {Seeded-{BTM}},
	doi = {10.1109/HPCC/SmartCity/DSS.2019.00386},
	abstract = {One of the most challenging problems in aspect level-analysis of product reviews is aspect mining (or aspect extraction), which aims to extract and categorize the terms that describe aspects of the product. In recent years, as topic models like LDA can perform extraction and clustering in one step, most unsupervised and semi-supervised statistical models are LDA-based. These models often treat each short sentence from the review as the training unit. However, LDA is proposed for normal documents without considering the sparse word frequency of short texts, which makes LDA perform poorly on short texts. Instead of using LDA, this paper proposes a Seeded Biterm Topic Model (Seeded-BTM) that models the co-occurred word pairs (i.e., biterms) rather than sentences or whole reviews. And by using seed sets, the model enables unsupervised BTM to discover aspect topics that under the user guidance. Experimental results on real-world product reviews from a number of domains show that Seeded-BTM can find more human conformable product aspects and outperforms the state-of-the-art models.},
	booktitle = {2019 {IEEE} 21st {International} {Conference} on {High} {Performance} {Computing} and {Communications}; {IEEE} 17th {International} {Conference} on {Smart} {City}; {IEEE} 5th {International} {Conference} on {Data} {Science} and {Systems} ({HPCC}/{SmartCity}/{DSS})},
	author = {Li, Ning and Chow, Chi-Yin and Zhang, Jia-Dong},
	month = aug,
	year = {2019},
	keywords = {Aspect Mining, Data mining, Data models, Hidden Markov models, Keyboards, Portable computers, Seeded Biterm Topic Model, Task analysis, Topic Model, Training},
	pages = {2751--2758},
}

@article{ferner_automated_2020,
	title = {Automated {Seeded} {Latent} {Dirichlet} {Allocation} for {Social} {Media} {Based} {Event} {Detection} and {Mapping}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/11/8/376},
	doi = {10.3390/info11080376},
	abstract = {In the event of a natural disaster, geo-tagged Tweets are an immediate source of information for locating casualties and damages, and for supporting disaster management. Topic modeling can help in detecting disaster-related Tweets in the noisy Twitter stream in an unsupervised manner. However, the results of topic models are difficult to interpret and require manual identification of one or more “disaster topics”. Immediate disaster response would benefit from a fully automated process for interpreting the modeled topics and extracting disaster relevant information. Initializing the topic model with a set of seed words already allows to directly identify the corresponding disaster topic. In order to enable an automated end-to-end process, we automatically generate seed words using older Tweets from the same geographic area. The results of two past events (Napa Valley earthquake 2014 and hurricane Harvey 2017) show that the geospatial distribution of Tweets identified as disaster related conforms with the officially released disaster footprints. The suggested approach is applicable when there is a single topic of interest and comparative data available.},
	language = {en},
	number = {8},
	urldate = {2022-09-23},
	journal = {Information},
	author = {Ferner, Cornelia and Havas, Clemens and Birnbacher, Elisabeth and Wegenkittl, Stefan and Resch, Bernd},
	month = aug,
	year = {2020},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {disaster management, geospatial analysis, social media, topic modeling},
	pages = {376},
	file = {Full Text PDF:/Users/nelsonquintanilla/Zotero/storage/GEQQCBS2/Ferner et al. - 2020 - Automated Seeded Latent Dirichlet Allocation for S.pdf:application/pdf;Snapshot:/Users/nelsonquintanilla/Zotero/storage/R29AH2LW/376.html:text/html},
}
@article{albalawi_using_2020,
	title = {Using {Topic} {Modeling} {Methods} for {Short}-{Text} {Data}: {A} {Comparative} {Analysis}},
	volume = {3},
	issn = {2624-8212},
	shorttitle = {Using {Topic} {Modeling} {Methods} for {Short}-{Text} {Data}},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2020.00042},
	abstract = {With the growth of online social network platforms and applications, large amounts of textual user-generated content are created daily in the form of comments, reviews, and short-text messages. As a result, users often find it challenging to discover useful information or more on the topic being discussed from such content. Machine learning and natural language processing algorithms are used to analyze the massive amount of textual social media data available online, including topic modeling techniques that have gained popularity in recent years. This paper investigates the topic modeling subject and its common application areas, methods, and tools. Also, we examine and compare five frequently used topic modeling methods, as applied to short textual social data, to show their benefits practically in detecting important topics. These methods are latent semantic analysis, latent Dirichlet allocation, non-negative matrix factorization, random projection, and principal component analysis. Two textual datasets were selected to evaluate the performance of included topic modeling methods based on the topic quality and some standard statistical evaluation metrics, like recall, precision, F-score, and topic coherence. As a result, latent Dirichlet allocation and non-negative matrix factorization methods delivered more meaningful extracted topics and obtained good results. The paper sheds light on some common topic modeling methods in a short-text context and provides direction for researchers who seek to apply these methods.},
	urldate = {2022-09-23},
	journal = {Frontiers in Artificial Intelligence},
	author = {Albalawi, Rania and Yeap, Tet Hin and Benyoucef, Morad},
	year = {2020},
	file = {Full Text PDF:/Users/nelsonquintanilla/Zotero/storage/JUXDT3AP/Albalawi et al. - 2020 - Using Topic Modeling Methods for Short-Text Data .pdf:application/pdf},
}


@article{towne_measuring_2016,
	title = {Measuring {Similarity} {Similarly}: {LDA} and {Human} {Perception}},
	volume = {8},
	issn = {2157-6904},
	shorttitle = {Measuring {Similarity} {Similarly}},
	url = {https://doi.org/10.1145/2890510},
	doi = {10.1145/2890510},
	abstract = {Several intelligent technologies designed to improve navigability in and digestibility of text corpora use topic modeling such as the state-of-the-art Latent Dirichlet Allocation (LDA). This model and variants on it provide lower-dimensional document representations used in visualizations and in computing similarity between documents. This article contributes a method for validating such algorithms against human perceptions of similarity, especially applicable to contexts in which the algorithm is intended to support navigability between similar documents via dynamically generated hyperlinks. Such validation enables researchers to ground their methods in context of intended use instead of relying on assumptions of fit. In addition to the methodology, this article presents the results of an evaluation using a corpus of short documents and the LDA algorithm. We also present some analysis of potential causes of differences between cases in which this model matches human perceptions of similarity more or less well.},
	number = {1},
	urldate = {2022-09-23},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Towne, W. Ben and Rosé, Carolyn P. and Herbsleb, James D.},
	month = sep,
	year = {2016},
	keywords = {algorithm validation, Perceived similarity, similarity metrics},
	pages = {7:1--7:28},
	file = {Full Text PDF:/Users/nelsonquintanilla/Zotero/storage/S5BSRKCC/Towne et al. - 2016 - Measuring Similarity Similarly LDA and Human Perc.pdf:application/pdf},
}

