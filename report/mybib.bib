@article{blei_probabilistic_2012,
	title = {Probabilistic topic models},
	volume = {55},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2133806.2133826},
	doi = {10.1145/2133806.2133826},
	abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
	language = {en},
	number = {4},
	urldate = {2022-09-16},
	journal = {Communications of the ACM},
	author = {Blei, David M.},
	month = apr,
	year = {2012},
	pages = {77--84},
	file = {Blei - 2012 - Probabilistic topic models.pdf:/Users/nelsonquintanilla/Zotero/storage/I2J2PVHI/Blei - 2012 - Probabilistic topic models.pdf:application/pdf},
}

@article{blei_probabilistic_2010,
	title = {Probabilistic {Topic} {Models}},
	volume = {27},
	issn = {1558-0792},
	doi = {10.1109/MSP.2010.938079},
	abstract = {In this article, we review probabilistic topic models: graphical models that can be used to summarize a large collection of documents with a smaller number of distributions over words. Those distributions are called "topics" because, when fit to data, they capture the salient themes that run through the collection. We describe both finite-dimensional parametric topic models and their Bayesian nonparametric counterparts, which are based on the hierarchical Dirichlet process (HDP). We discuss two extensions of topic models to time-series data-one that lets the topics slowly change over time and one that lets the assumed prevalence of the topics change. Finally, we illustrate the application of topic models to nontext data, summarizing some recent research results in image analysis.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Blei, David and Carin, Lawrence and Dunson, David},
	month = nov,
	year = {2010},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Analytical models, Bayesian methods, Computational modeling, Data models, Graphical models, Markov processes},
	pages = {55--65},
	file = {Blei - Probabilistic topic models.pdf:/Users/nelsonquintanilla/Zotero/storage/MAPDH4FW/Blei - Probabilistic topic models.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/nelsonquintanilla/Zotero/storage/XL5BIT9W/5563111.html:text/html;IEEE Xplore Full Text PDF:/Users/nelsonquintanilla/Zotero/storage/4CXA3DRX/Blei et al. - 2010 - Probabilistic Topic Models.pdf:application/pdf},
}

@article{blei_latent_nodate,
	title = {Latent {Dirichlet} {Allocation}},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a ﬁnite mixture over an underlying set of topics. Each topic is, in turn, modeled as an inﬁnite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efﬁcient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classiﬁcation, and collaborative ﬁltering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	language = {en},
	author = {Blei, David M},
	pages = {30},
	file = {Blei - Latent Dirichlet Allocation.pdf:/Users/nelsonquintanilla/Zotero/storage/VXMXVDHN/Blei - Latent Dirichlet Allocation.pdf:application/pdf},
}

@inproceedings{sievert_ldavis_2014,
	address = {Baltimore, Maryland, USA},
	title = {{LDAvis}: {A} method for visualizing and interpreting topics},
	shorttitle = {{LDAvis}},
	url = {http://aclweb.org/anthology/W14-3110},
	doi = {10.3115/v1/W14-3110},
	abstract = {We present LDAvis, a web-based interactive visualization of topics estimated using Latent Dirichlet Allocation that is built using a combination of R and D3. Our visualization provides a global view of the topics (and how they differ from each other), while at the same time allowing for a deep inspection of the terms most highly associated with each individual topic. First, we propose a novel method for choosing which terms to present to a user to aid in the task of topic interpretation, in which we deﬁne the relevance of a term to a topic. Second, we present results from a user study that suggest that ranking terms purely by their probability under a topic is suboptimal for topic interpretation. Last, we describe LDAvis, our visualization system that allows users to ﬂexibly explore topic-term relationships using relevance to better understand a ﬁtted LDA model.},
	language = {en},
	urldate = {2022-08-31},
	booktitle = {Proceedings of the {Workshop} on {Interactive} {Language} {Learning}, {Visualization}, and {Interfaces}},
	publisher = {Association for Computational Linguistics},
	author = {Sievert, Carson and Shirley, Kenneth},
	year = {2014},
	pages = {63--70},
	file = {Sievert and Shirley - 2014 - LDAvis A method for visualizing and interpreting .pdf:/Users/nelsonquintanilla/Zotero/storage/E4S6JPWG/Sievert and Shirley - 2014 - LDAvis A method for visualizing and interpreting .pdf:application/pdf},
}

@article{jacobi_quantitative_2016,
	title = {Quantitative analysis of large amounts of journalistic texts using topic modelling},
	volume = {4},
	issn = {2167-0811, 2167-082X},
	url = {http://www.tandfonline.com/doi/full/10.1080/21670811.2015.1093271},
	doi = {10.1080/21670811.2015.1093271},
	language = {en},
	number = {1},
	urldate = {2022-09-17},
	journal = {Digital Journalism},
	author = {Jacobi, Carina and van Atteveldt, Wouter and Welbers, Kasper},
	month = jan,
	year = {2016},
	keywords = {automatic content analysis, Corrigendum, journalism, nuclear energy, topic models},
	pages = {89--106},
	file = {Jacobi et al. - 2016 - Quantitative analysis of large amounts of journali.pdf:/Users/nelsonquintanilla/Zotero/storage/GM26PM6G/Jacobi et al. - 2016 - Quantitative analysis of large amounts of journali.pdf:application/pdf;Snapshot:/Users/nelsonquintanilla/Zotero/storage/F4NA2GZ4/21670811.2015.html:text/html},
}

@article{caldara_effect_2022,
	title = {The {Effect} of the {War} in {Ukraine} on {Global} {Activity} and {Inflation}},
	url = {https://www.federalreserve.gov/econres/notes/feds-notes/the-effect-of-the-war-in-ukraine-on-global-activity-and-inflation-20220527.html},
	abstract = {The Federal Reserve Board of Governors in Washington DC.},
	language = {en},
	urldate = {2022-09-17},
	author = {Caldara, Dario and Conlisk, Sarah and Iacoviello, Matteo and Penn, Maddie},
	month = may,
	year = {2022},
	file = {Snapshot:/Users/nelsonquintanilla/Zotero/storage/C76JDUYI/the-effect-of-the-war-in-ukraine-on-global-activity-and-inflation-20220527.html:text/html},
}

@article{jelodar_latent_2019,
	title = {Latent {Dirichlet} allocation ({LDA}) and topic modeling: models, applications, a survey},
	volume = {78},
	issn = {1380-7501, 1573-7721},
	shorttitle = {Latent {Dirichlet} allocation ({LDA}) and topic modeling},
	url = {http://link.springer.com/10.1007/s11042-018-6894-4},
	doi = {10.1007/s11042-018-6894-4},
	abstract = {Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data and text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modelling; Latent Dirichlet Allocation (LDA) is one of the most popular in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper will be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated highly scholarly articles (between 2003 to 2016) related to topic modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. In addition, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA.},
	language = {en},
	number = {11},
	urldate = {2022-09-17},
	journal = {Multimedia Tools and Applications},
	author = {Jelodar, Hamed and Wang, Yongli and Yuan, Chi and Feng, Xia and Jiang, Xiahui and Li, Yanchao and Zhao, Liang},
	month = jun,
	year = {2019},
	pages = {15169--15211},
	file = {Jelodar et al. - 2019 - Latent Dirichlet allocation (LDA) and topic modeli.pdf:/Users/nelsonquintanilla/Zotero/storage/NW5KNSTF/Jelodar et al. - 2019 - Latent Dirichlet allocation (LDA) and topic modeli.pdf:application/pdf},
}

@article{nikolenko_topic_2017,
	title = {Topic modelling for qualitative studies},
	volume = {43},
	issn = {0165-5515},
	url = {https://doi.org/10.1177/0165551515617393},
	doi = {10.1177/0165551515617393},
	abstract = {Qualitative studies, such as sociological research, opinion analysis and media studies, can benefit greatly from automated topic mining provided by topic models such as latent Dirichlet allocation (LDA). However, examples of qualitative studies that employ topic modelling as a tool are currently few and far between. In this work, we identify two important problems along the way to using topic models in qualitative studies: lack of a good quality metric that closely matches human judgement in understanding topics and the need to indicate specific subtopics that a specific qualitative study may be most interested in mining. For the first problem, we propose a new quality metric, tf-idf coherence, that reflects human judgement more accurately than regular coherence, and conduct an experiment to verify this claim. For the second problem, we propose an interval semi-supervised approach (ISLDA) where certain predefined sets of keywords (that define the topics researchers are interested in) are restricted to specific intervals of topic assignments. Our experiments show that ISLDA is better for topic extraction than LDA in terms of tf-idf coherence, number of topics identified to predefined keywords and topic stability. We also present a case study on a Russian LiveJournal dataset aimed at ethnicity discourse analysis.},
	language = {en},
	number = {1},
	urldate = {2022-07-11},
	journal = {Journal of Information Science},
	author = {Nikolenko, Sergey I. and Koltcov, Sergei and Koltsova, Olessia},
	month = feb,
	year = {2017},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {Latent Dirichlet allocation, LDA extensions, topic modelling, topic quality},
	pages = {88--102},
	file = {SAGE PDF Full Text:/Users/nelsonquintanilla/Zotero/storage/QXV8NXVP/Nikolenko et al. - 2017 - Topic modelling for qualitative studies.pdf:application/pdf},
}

@article{chang_reading_nodate,
	title = {Reading {Tea} {Leaves}: {How} {Humans} {Interpret} {Topic} {Models}},
	abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
	language = {en},
	author = {Chang, Jonathan and Boyd-Graber, Jordan and Gerrish, Sean and Wang, Chong and Blei, David M},
	pages = {10},
	file = {Chang et al. - Reading Tea Leaves How Humans Interpret Topic Mod.pdf:/Users/nelsonquintanilla/Zotero/storage/8W2NEUPZ/Chang et al. - Reading Tea Leaves How Humans Interpret Topic Mod.pdf:application/pdf},
}

@misc{matsa_western_2018,
	title = {Western {Europeans} {Under} 30 {View} {News} {Media} {Less} {Positively}, {Rely} {More} on {Digital} {Platforms} {Than} {Older} {Adults}},
	url = {https://www.pewresearch.org/journalism/2018/10/30/western-europeans-under-30-view-news-media-less-positively-rely-more-on-digital-platforms-than-older-adults/},
	abstract = {Younger adults in eight Western European countries are about twice as likely as older adults to get news online than from TV. They also are more critical of the media's performance and coverage of key issues.},
	language = {en-US},
	urldate = {2022-09-22},
	journal = {Pew Research Center's Journalism Project},
	author = {Matsa, Katerina Eva and Silver, Laura and Shearer, Elisa and Walker, Mason},
	month = oct,
	year = {2018},
	file = {Snapshot:/Users/nelsonquintanilla/Zotero/storage/BYK48N7C/western-europeans-under-30-view-news-media-less-positively-rely-more-on-digital-platforms-than-.html:text/html},
}

@misc{noauthor_theguardian_nodate,
	title = {theguardian / open platform - documentation / overview},
	url = {https://open-platform.theguardian.com/documentation/},
	urldate = {2022-09-22},
	file = {theguardian / open platform - documentation / overview:/Users/nelsonquintanilla/Zotero/storage/UXCUQDHN/documentation.html:text/html},
}

@article{jacobi_quantitative_2016,
	title = {Quantitative analysis of large amounts of journalistic texts using topic modelling},
	volume = {4},
	issn = {2167-0811, 2167-082X},
	url = {http://www.tandfonline.com/doi/full/10.1080/21670811.2015.1093271},
	doi = {10.1080/21670811.2015.1093271},
	language = {en},
	number = {1},
	urldate = {2022-09-17},
	journal = {Digital Journalism},
	author = {Jacobi, Carina and van Atteveldt, Wouter and Welbers, Kasper},
	month = jan,
	year = {2016},
	keywords = {automatic content analysis, Corrigendum, journalism, nuclear energy, topic models},
	pages = {89--106},
	file = {Jacobi et al. - 2016 - Quantitative analysis of large amounts of journali.pdf:/Users/nelsonquintanilla/Zotero/storage/GM26PM6G/Jacobi et al. - 2016 - Quantitative analysis of large amounts of journali.pdf:application/pdf;Snapshot:/Users/nelsonquintanilla/Zotero/storage/F4NA2GZ4/21670811.2015.html:text/html},
}

@article{bird_natural_nodate,
	title = {Natural {Language} {Processing} with {Python}},
	language = {en},
	author = {Bird, Steven},
	pages = {504},
	file = {Bird - Natural Language Processing with Python.pdf:/Users/nelsonquintanilla/Zotero/storage/D558Z82Q/Bird - Natural Language Processing with Python.pdf:application/pdf},
}

@article{pedregosa_scikit-learn_nodate,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simpliﬁed BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	language = {en},
	journal = {MACHINE LEARNING IN PYTHON},
	author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David},
	pages = {6},
	file = {Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf:/Users/nelsonquintanilla/Zotero/storage/DFYC4M3V/Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf:application/pdf},
}

@unpublished{spacy2,
    AUTHOR = {Honnibal, Matthew and Montani, Ines},
    TITLE  = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing},
    YEAR   = {2017},
    Note   = {To appear}
}

@book{rajaraman_mining_2011,
	address = {USA},
	title = {Mining of {Massive} {Datasets}},
	isbn = {978-1-107-01535-7},
	abstract = {The popularity of the Web and Internet commerce provides many extremely large datasets from which information can be gleaned by data mining. This book focuses on practical algorithms that have been used to solve key problems in data mining and which can be used on even the largest datasets. It begins with a discussion of the map-reduce framework, an important tool for parallelizing algorithms automatically. The authors explain the tricks of locality-sensitive hashing and stream processing algorithms for mining data that arrives too fast for exhaustive processing. The PageRank idea and related tricks for organizing the Web are covered next. Other chapters cover the problems of finding frequent itemsets and clustering. The final chapters cover two applications: recommendation systems and Web advertising, each vital in e-commerce. Written by two authorities in database and Web technologies, this book is essential reading for students and practitioners alike.},
	publisher = {Cambridge University Press},
	author = {Rajaraman, Anand and Ullman, Jeffrey David},
	year = {2011},
}

@article{rehurek_gensimstatistical_nodate,
	title = {Gensim—{Statistical} {Semantics} in {Python}},
	abstract = {Gensim is a pure Python library that fights on two fronts: 1) digital document indexing and similarity search; and 2) fast, memory-efficient, scalable algorithms for Singular Value Decomposition and Latent Dirichlet Allocation. The connection between the two is unsupervised, semantic analysis of plain text in digital collections. Gensim was created for large digital libraries, but its underlying algorithms for large-scale, distributed, online SVD and LDA are like the Swiss Army knife of data analysis—also useful on their own, outside of the domain of Natural Language Processing.},
	language = {en},
	author = {Řehůřek, Radim and Sojka, Petr},
	pages = {1},
	file = {Řehůřek and Sojka - Gensim—Statistical Semantics in Python.pdf:/Users/nelsonquintanilla/Zotero/storage/7VKWSNHU/Řehůřek and Sojka - Gensim—Statistical Semantics in Python.pdf:application/pdf},
}

@INPROCEEDINGS{Rehurek10softwareframework,
    author = {Radim Rehurek and Petr Sojka},
    title = {Software Framework for Topic Modelling with Large Corpora},
    booktitle = {IN PROCEEDINGS OF THE LREC 2010 WORKSHOP ON NEW CHALLENGES FOR NLP FRAMEWORKS},
    year = {2010},
    pages = {45--50},
    publisher = {}
}

@inproceedings{2009-topic-modeling-social-sciences,
  title = {Topic Modeling for the Social Sciences},
  author = {Daniel Ramage AND Evan Rosen AND Jason Chuang AND Christopher D. Manning AND Daniel A. McFarland},
  booktitle = {Workshop on Applications for Topic Models, NIPS},
  year = {2009},
  url = {http://vis.stanford.edu/papers/topic-modeling-social-sciences}
}

@unpublished{McCallumMALLET,
      author = "Andrew Kachites McCallum",
      title = "MALLET: A Machine Learning for Language Toolkit",
      note = "http://www.cs.umass.edu/~mccallum/mallet",
      year = 2002}
      
@article{blei_probabilistic_2010,
	title = {Probabilistic {Topic} {Models}},
	volume = {27},
	issn = {1558-0792},
	doi = {10.1109/MSP.2010.938079},
	abstract = {In this article, we review probabilistic topic models: graphical models that can be used to summarize a large collection of documents with a smaller number of distributions over words. Those distributions are called "topics" because, when fit to data, they capture the salient themes that run through the collection. We describe both finite-dimensional parametric topic models and their Bayesian nonparametric counterparts, which are based on the hierarchical Dirichlet process (HDP). We discuss two extensions of topic models to time-series data-one that lets the topics slowly change over time and one that lets the assumed prevalence of the topics change. Finally, we illustrate the application of topic models to nontext data, summarizing some recent research results in image analysis.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Blei, David and Carin, Lawrence and Dunson, David},
	month = nov,
	year = {2010},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Analytical models, Bayesian methods, Computational modeling, Data models, Graphical models, Markov processes},
	pages = {55--65},
	file = {IEEE Xplore Abstract Record:/Users/nelsonquintanilla/Zotero/storage/CQTEH3T8/5563111.html:text/html;IEEE Xplore Full Text PDF:/Users/nelsonquintanilla/Zotero/storage/9RZMGA3J/Blei et al. - 2010 - Probabilistic Topic Models.pdf:application/pdf},
}



